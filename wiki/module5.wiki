#Units of Memory Bit (Binary Digit), Byte, Kilobyte, Megabyte, Gigabyte, Terabyte Petabyte

=*Memory*=

Memory is the ability to store, retain, and recall information.

==*Memory Bit (Binary Digit)*==

A bit is a variable or computed quantity that can have only two possible values. These two values are often interpreted as binary digits and are usually denoted by the Arabic numerical digits 0 and 1. Indeed, the term "bit" is a contraction of binary digit. The two values can also be interpreted as logical values (true/false, yes/no). algebraic signs (+/−), activation states (on/off), or any other two-valued attribute. In several popular programming languages, numeric 0 is equivalent (or convertible) to logical false, and 1 to true. The correspondence between these values and the physical states of the underlying storage or device is a matter of convention, and different assignments may be used even within the same device or program.

The symbol for bit, as a unit of information, is "bit" or (lowercase) "b"; the latter being recommended by the IEEE 1541 Standard (2002).

===*History*===

The encoding of data by discrete bits was used in the punched cards invented by Basile Bouchon and Jean-Baptiste Falcon (1725), developed by Joseph Marie Jacquard (1804), and later adopted by Semen Korsakov, Charles Babbage, Hermann Hollerith, and early computer manufacturers like IBM. Another variant of that idea was the perforated paper tape. In all those systems, the medium (card or tape) conceptually carried an array of hole positions; each position could be either punched through or not, thus potentially carrying one bit of information. The encoding of text by bits was also used in Morse code (1844) and early digital communications machines such as teletypes and stock ticker machines (1870).

Ralph Hartley suggested the use of a logarithmic measure of information in 1928. Claude E. Shannon first used the word bit in his seminal 1948 paper A Mathematical Theory of Communication. He attributed its origin to John W. Tukey, who had written a Bell Labs memo on 9 January 1947 in which he contracted "binary digit" to simply "bit". Interestingly, Vannevar Bush had written in 1936 of "bits of information" that could be stored on the punch cards used in the mechanical computers of that time. The first programmable computer built by Konrad Zuse used binary notation for numbers.

==*Byte (B)*==
The byte (pronounced /ˈbaɪt/) is a unit of digital information in computing and telecommunications. It is an ordered collection of bits, in which each bit denotes the binary value of 1 or 0. Historically, a byte was the number of bits (typically 6, 7, 8, or 9) used to encode a character of text in a computer and it is for this reason the basic addressable element in many computer architectures. The size of a byte is typically hardware dependent, but the modern de facto standard is 8 bits, as this is a convenient power of 2. Most of the numeric values used by many applications are representable in 8 bits and processor designers optimize for this common usage. Signal processing applications tend to operate on larger values and some digital signal processors have 16 or 40 bits as the smallest unit of addressable storage (on such processors a byte may be defined to contain this number of bits).

The term octet was explicitly defined to denote a sequence of 8 bits because of the ambiguity associated with the term byte and is widely used in communications protocol specifications.


==*Kilobyte (KB)*==
The kilobyte is a multiple of the unit byte for digital information storage or transmission. The prefix kilo means 1000 in the International System of Units (SI), therefore 1 kilobyte is 1000 bytes. The recommended unit symbol for the kilobyte is kB or kbyte. Historically the unit has also been used to denote 1024 (210) bytes, as digital system are based on multiples of powers of 2. This use however, has been discouraged by standards organizations and a new unit was created, the kibibyte, as replacement for the binary multiple.

==*Megabyte (MB)*==
The megabyte is a multiple of the unit byte for digital information storage or transmission with two different values depending on context: 1048576 bytes (10242) generally for computer memory; and one million bytes generally for computer storage. The IEEE Standards Board has decided that "Mega will mean 1 000 000", with exceptions allowed for the base-two meaning. In rare cases, it is used to mean 1000×1024 (1024000) bytes. It is commonly abbreviated as Mbyte or MB (compare Mb, for the megabit).

==*Gigabyte (GB)*==
The gigabyte is a multiple of the unit byte for digital information storage. The prefix giga means ,,10,,9 in the International System of Units (SI), therefore 1 gigabyte is 1000000000 bytes. The unit symbol for the gigabyte is GB or Gbyte, but not Gb (lower case b) which is typically used for the gigabit.

==*Terabyte (TB)*==
A terabyte is a SI-multiple (see prefix tera) of the unit byte for digital information storage and is equal to 10>12 (1 trillion short scale) bytes or 1000 gigabytes. The unit symbol for the terabyte is TB.

The designation terabyte is rarely used to refer to the tebibyte, its binary prefix analogue, because only recent (since 2007) disk drives have this capacity. Disk drive sizes are always designated in SI units by manufacturers. However, a possible confusion arises from this definition with the long-standing tradition in some fields of information technology and the computer industry of using binary prefix interpretations for memory sizes. Standards organizations such as International Electrotechnical Commission (IEC), Institute of Electrical and Electronics Engineers (IEEE) and International Organization for Standardization (ISO) recommend to use the alternative term tebibyte to signify the traditional measure of 10244 bytes, or 1024 gibibytes, leading to the following definitions:

In standard SI usage, 1 terabyte (TB) equals 1000000000000 bytes = 1000>4 or 10>12 bytes.
Using the traditional binary interpretation, a terabyte is 1099511627776bytes = 1024>4 = 2>40 bytes = 1 tebibyte (TiB).
The capacities of computer storage devices are typically specified using their the standard SI meaning of unit prefixes, but many operating systems and applications report in binary-based units. Mac OS X 10.6 (Snow Leopard) reports decimal units.

==*Petabyte (TB)*==
A petabyte (derived from the SI prefix peta- ) is a unit of information or computer storage equal to one quadrillion bytes (short scale), or 1000 terabytes, or 1,000,000 gigabytes. It is abbreviated PB. The prefix peta- (P) indicates a power of 1000:

1 PB = 1,000,000,000,000,000 B = 10005 B = 1015 bytes.
The term "pebibyte", using the binary prefix pebi- (Pi), is used for 1024^5^ bytes.

1 PiB = 1,125,899,906,842,624 B.